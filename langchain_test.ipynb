{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './llama-2-7b-chat.ggmlv3.q5_1.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./llama-2-7b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 5346.60 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/ngoc.nguyenluu/.pyenv/versions/3.10.4/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x123e99420\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100b0fdb0\n",
      "ggml_metal_init: loaded kernel_mul                            0x123e9b000\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x123e9aa30\n",
      "ggml_metal_init: loaded kernel_scale                          0x123e9b410\n",
      "ggml_metal_init: loaded kernel_silu                           0x123e9cf50\n",
      "ggml_metal_init: loaded kernel_relu                           0x167372230\n",
      "ggml_metal_init: loaded kernel_gelu                           0x123e9c570\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x123e9d850\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x123e9ed70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x123e9e1b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x123e9e6e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x123ea0470\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x123a5bc00\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1673731a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x123b50c30\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x167b4e9c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x167b4e430\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x167b4f0d0\n",
      "ggml_metal_init: loaded kernel_norm                           0x167b50190\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x123e9f890\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x123ea0d50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x123ea1310\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x123ea1ce0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x123ea26c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x123ea3880\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x123ea4290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x167b50fb0\n",
      "ggml_metal_init: loaded kernel_rope                           0x167b518b0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x123ea4ca0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x123ea55b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x167b528e0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x167b4f820\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    93.75 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  4820.95 MB, ( 4821.41 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 4831.58 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  2050.00 MB, ( 6881.58 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   356.00 MB, ( 7237.58 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 7397.58 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH, \n",
    "    n_ctx=4096, \n",
    "    use_mlock=True, \n",
    "    f16_kv=True, \n",
    "    n_threads=8,\n",
    ")\n",
    "# chat = AzureChatOpenAI(deployment_name = 'gpt-4',\n",
    "#     temperature=0.0,\n",
    "#     openai_api_key = <api_key>,\n",
    "#     openai_api_base = \"[https://chatbot-public-api.svc.stg-catwalk-2.chimera.myteksi.net](https://chatbot-public-api.svc.stg-catwalk-2.chimera.myteksi.net/)\",\n",
    "#     openai_api_version = \"2023-03-15-preview\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
    "\n",
    "Title: {title}\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
    "\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Tragedy at sunset on the beach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The latest offering from the Off-Broadway stage, Tragedy at Sunset on the Beach is a poignant and timely examination of the fragility of human relationships. With its minimalist setting and spare dialogue, the play immerses the audience in the intimate details of two friends' reunion as they confront long-buried secrets and past conflicts.\n",
      "\n",
      "The performances of the cast are superb, with standout work from (actor name) as Jack, who brings a deep sense of vulnerability to his portrayal of grief. The chemistry between the two leads is palpable, making their escalating tensions all the more intense and compelling.\n",
      "\n",
      "Playwright (playwright's name) skillfully weaves together flashbacks of the friends' shared history to reveal the complex dynamics that have shaped their friendship over time. Through these fragmented glimpses into the past, the play delves into the heartbreak and resentments that can accumulate in even the closest of relationships.\n",
      "\n",
      "Ultimately, Tragedy at Sunset on the Beach is a poignant meditation on mortality and the fragility\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
